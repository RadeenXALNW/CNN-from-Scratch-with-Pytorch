{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "STL10 NEW 1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "onsV2ZHOU3rj"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import  torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets import STL10\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data.dataloader import DataLoader,Dataset\n",
        "from torch.utils.data import random_split\n",
        "from torchvision import transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLLmDEx7VBnG"
      },
      "source": [
        "class Conv_operation:\n",
        "    def __init__(self,num_filters,filter_size,stride_size,padding_size):\n",
        "        self.num_filters=num_filters\n",
        "        self.filter_size=filter_size\n",
        "        self.padding_size=padding_size\n",
        "        self.stride_size=stride_size\n",
        "        self.conv_filter=torch.rand(num_filters,filter_size,filter_size)/(filter_size*filter_size)\n",
        "    #image patching \n",
        "    def image_region(self,image):\n",
        "        height,width=image.shape\n",
        "        self.image=image\n",
        "        for j in range((height-self.filter_size)+1):\n",
        "            for k in range((width-self.filter_size)+1):\n",
        "                image_patch=image[j:(j+self.filter_size),k:(k+self.filter_size)]\n",
        "                yield image_patch,j,k\n",
        "    def forward_prop(self,image):\n",
        "#         assert single_sample.dim()==3, f'Input not 2D, given {single_sample.dim()}D'\n",
        "        # image=torch.squeeze(image)\n",
        "        height,width=image.shape\n",
        "        padding_size=(self.filter_size-1)//2\n",
        "        conv_out=torch.zeros(((height-self.filter_size+2*padding_size)//self.stride_size)+1,((width-self.filter_size+2*padding_size)//self.stride_size)+1,self.num_filters)\n",
        "        for image_path,i,j in self.image_region(image):\n",
        "            conv_out[i,j]= torch.sum(image_path*self.conv_filter)\n",
        "        # conv_out = 1. / (1. + torch.exp(-conv_out))\n",
        "        return conv_out\n",
        "    def padding_size(self):\n",
        "        return(self.filter_size-1)//2\n",
        "\n",
        "        \n",
        "    def relu(self,xa,derive=False):\n",
        "      if derive:\n",
        "        return torch.ceil(torch.clamp(xa,min=0,max=1)).detach\n",
        "      return torch.clamp(xa,min=0).detach()\n",
        "\n",
        "\n",
        "    def backward_prop(self,d_L_dout,learning_rate):\n",
        "        dL_dF_params=torch.zeros(self.conv_filter.shape)\n",
        "        for image_patch,i,j in self.image_region(self.image):\n",
        "            for k in range(self.num_filters):\n",
        "                dL_dF_params[k]+=image_patch*d_L_dout[i,j,k]\n",
        "#         learning_rate=torch.tensor(learning_rate)\n",
        "#         dL_dF_params=torch.tensor(dL_dF_params)\n",
        "        self.conv_filter-=learning_rate*dL_dF_params\n",
        "        return dL_dF_params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzqZ0jiQVEE1"
      },
      "source": [
        "class Maxpooling:\n",
        "    def __init__(self,filter_size):\n",
        "        self.filter_size=filter_size\n",
        "    def image_region(self,image):\n",
        "        new_height=image.shape[0]//self.filter_size\n",
        "        new_width=image.shape[1]//self.filter_size\n",
        "        self.image=image\n",
        "        for i in range(new_height):\n",
        "\n",
        "\n",
        "          for j in range(new_width):\n",
        "\n",
        "\n",
        "            image_patch=image[(i*self.filter_size):(i*self.filter_size+self.filter_size),\n",
        "                                 (j*self.filter_size):(j*self.filter_size+self.filter_size)]\n",
        "            yield image_patch,i,j\n",
        "    def forward_prop(self,image):\n",
        "\n",
        "      height,width,num_filters=image.shape\n",
        "      output=torch.zeros(height//self.filter_size,width//self.filter_size,num_filters)\n",
        "      \n",
        "      for image_patch, i, j in self.image_region(image):\n",
        "        # image_patch=torch.flatten(image_patch,start_dim=0,end_dim=1)\n",
        "        output[i,j]=torch.amax(image_patch)\n",
        "      return output\n",
        "    \n",
        "\n",
        "    def backward_prop(self,d_L_dout): #dL_dout is the input from softmax layer\n",
        "      dl_dinput=torch.zeros(self.image.shape)\n",
        "      for image_patch,i,j in self.image_region(self.image):\n",
        "        height,width,num_filters=image_patch.shape\n",
        "        # max_val=torch.max(image_patch,dim=0)\n",
        "\n",
        "  \n",
        "            \n",
        "        for i1 in range(height):\n",
        "\n",
        "          for j1 in range(width):\n",
        "\n",
        "                for k1 in range(num_filters):\n",
        "                  x_pool=dl_dinput[i*self.filter_size+i1,j*self.filter_size+j1,k1]\n",
        "                  mask=(x_pool==torch.amax(x_pool))\n",
        "                  dl_dinput[i*self.filter_size+i1,j*self.filter_size+j1,k1]=mask*d_L_dout[i,j,k1]\n",
        "                                  \n",
        "                           \n",
        "        return dl_dinput"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOwnqXvtVGyo"
      },
      "source": [
        "class Softmax:\n",
        "    def __init__(self,input_node,softmax_node):\n",
        "        self.weight=torch.randn(input_node,softmax_node)/input_node\n",
        "        self.bias=torch.zeros(softmax_node)\n",
        "    def forward_prop(self,image):\n",
        "        self.last_input_shape=image.shape\n",
        "        new_image=image.flatten()\n",
        "        self.modified_input=new_image #to be  used in backpropagation\n",
        "#         input_node,softmax_node=self.weight.shape\n",
        "#         new_image=torch.squeeze(new_image,dim=1)\n",
        "        new_image=new_image.cpu().detach().numpy()\n",
        "        self.weight=self.weight.cpu().detach().numpy()\n",
        "        self.bias=self.bias.cpu().detach().numpy()\n",
        "        output_val=np.dot(new_image,self.weight)+self.bias\n",
        "        self.weight=torch.from_numpy(self.weight)\n",
        "        self.bias=torch.from_numpy(self.bias)\n",
        "      \n",
        "        output_val=torch.from_numpy(output_val)\n",
        "        self.out=output_val\n",
        "        exp_out=torch.exp(output_val)\n",
        "        return exp_out/torch.sum(exp_out)\n",
        "    \n",
        "    def backward_prop(self,d_L_dout,learning_rate):\n",
        "        for i, gradient in enumerate(d_L_dout):\n",
        "            if gradient==0:\n",
        "                continue\n",
        "            \n",
        "    #out(c)=e^tc/summation(e^ti)\n",
        "    #   where S=summation(e^ti)\n",
        "        t_exp=torch.exp(self.out)\n",
        "        #SUM OF ALL e^totals\n",
        "        S=torch.sum(t_exp)\n",
        "        #gradients of output[i] against totals\n",
        "        dout_dt=-t_exp[i]*t_exp/(S**2)\n",
        "        dout_dt[i]=t_exp[i]*(S - t_exp[i]) / (S ** 2)\n",
        "        \n",
        "        #gradients of totals against weights,biases, input\n",
        "        dt_dw=self.modified_input\n",
        "        dt_db=1\n",
        "        dt_dinput=self.weight\n",
        "        \n",
        "        #gradients of loss against totals\n",
        "        dL_dt=gradient*dout_dt\n",
        "        \n",
        "        #gradients of loss against weights, biases and input\n",
        "#         dt_dw=torch.unsqueeze(dt_dw,dim=0)\n",
        "#         dl_dt=torch.unsqueeze(dl_dt,dim=0)\n",
        "        dl_dw=torch.matmul(dt_dw.unsqueeze(0).t(),dL_dt.unsqueeze(0))\n",
        "        dl_db=torch.mul(dL_dt,dt_db)\n",
        "        dl_dinput=torch.matmul(dt_dinput,dL_dt)\n",
        "        \n",
        "        #update weights biases\n",
        "        \n",
        "        self.weight-=torch.mul(learning_rate,dl_dw)\n",
        "        self.bias-=torch.mul(learning_rate,dl_db)\n",
        "        \n",
        "        # return dl_dinput.reshape(self.last_input_shape)\n",
        "        return torch.reshape(dl_dinput,self.last_input_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I73PNrvNVJMB"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "train_images = X_train[:1500]\n",
        "train_labels = y_train[:1500]\n",
        "test_images = X_test[:1500]\n",
        "test_labels = y_test[:1500]\n",
        "test_images=torch.tensor(test_images)\n",
        "test_lables=torch.tensor(test_labels)\n",
        "\n",
        "conv=Conv_operation(8,3,1,1)\n",
        "pool=Maxpooling(2)\n",
        "softmax=Softmax(14*14*8,10)\n",
        "\n",
        "def cnn_forward_prop(image,label):\n",
        "    out= conv.forward_prop((image/255) -0.5)\n",
        "    out=torch.tensor(out)\n",
        "    out=pool.forward_prop(out)\n",
        "    out=softmax.forward_prop(out)\n",
        "    \n",
        "    #calculate cross entropy loss and accuracy\n",
        "    loss=-torch.log(out[label])\n",
        "    acc=torch.where(torch.argmax(out)==label,1,0)\n",
        "    \n",
        "#     out_p=torch.argmax(out_p)\n",
        "#     label=label.float()\n",
        "#     out_p=out_p.float()*(label>0).float()\n",
        "#     accuracy_eval=out_p*(out_p==label).float()\n",
        "    \n",
        "#     accuracy_eval=1 if torch.argmax(out_p)==label else 0\n",
        "#     correct+=(out_p==label).sum().item()\n",
        "#     accuracy_eval=100*correct/total\n",
        "    \n",
        "    return out,loss,acc\n",
        "def train_cnn(image,label,learning_rate=0.000000005):\n",
        "    #forward\n",
        "    out,loss,acc=cnn_forward_prop(image,label)\n",
        "    #calculate initial gradient\n",
        "    gradient=torch.zeros(10)\n",
        "    gradient[label]=-1/out[label]\n",
        "    \n",
        "    #backward\n",
        "    grad_back=softmax.backward_prop(gradient,learning_rate)\n",
        "    grad_back=pool.backward_prop(grad_back)\n",
        "    grad_back=conv.backward_prop(grad_back,learning_rate)\n",
        "    return loss,acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqO12vsyVNTy",
        "outputId": "68986ac5-adc8-47c7-c501-ca895764108e"
      },
      "source": [
        "for epoch1 in range(2):\n",
        "  print('Epoch %d ->'% (epoch1 +1))\n",
        "  \n",
        "\n",
        "  # shuffle the training data\n",
        "  shuffle_data = torch.randperm(len(train_images))\n",
        "  train_images = train_images[shuffle_data]\n",
        "  train_labels = train_labels[shuffle_data]\n",
        "  train_images=torch.tensor(train_images)\n",
        "  test_images=torch.tensor(test_images)\n",
        "\n",
        "  \n",
        "\n",
        "  #training the CNN\n",
        "  loss = 0.0\n",
        "  num_correct = 0\n",
        "\n",
        "  for i, (im, label) in enumerate(zip(train_images, train_labels)):\n",
        "    if i % 100 == 0:\n",
        "      print('%d steps out of 100 steps: Average Loss %.3f and Accuracy: %d%%' %(i+1, loss/100, num_correct))\n",
        "      loss = 0\n",
        "      num_correct = 0\n",
        "\n",
        "    l1, acc = train_cnn(im, label)\n",
        "    loss += l1\n",
        "\n",
        "    num_correct +=acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 ->\n",
            "1 steps out of 100 steps: Average Loss 0.000 and Accuracy: 0%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  app.launch_new_instance()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "101 steps out of 100 steps: Average Loss 2.300 and Accuracy: 14%\n",
            "201 steps out of 100 steps: Average Loss 2.306 and Accuracy: 6%\n",
            "301 steps out of 100 steps: Average Loss 2.307 and Accuracy: 8%\n",
            "401 steps out of 100 steps: Average Loss 2.300 and Accuracy: 11%\n",
            "501 steps out of 100 steps: Average Loss 2.297 and Accuracy: 12%\n",
            "601 steps out of 100 steps: Average Loss 2.294 and Accuracy: 12%\n",
            "701 steps out of 100 steps: Average Loss 2.297 and Accuracy: 12%\n",
            "801 steps out of 100 steps: Average Loss 2.299 and Accuracy: 11%\n",
            "901 steps out of 100 steps: Average Loss 2.304 and Accuracy: 9%\n",
            "1001 steps out of 100 steps: Average Loss 2.291 and Accuracy: 10%\n",
            "1101 steps out of 100 steps: Average Loss 2.297 and Accuracy: 9%\n",
            "1201 steps out of 100 steps: Average Loss 2.302 and Accuracy: 9%\n",
            "1301 steps out of 100 steps: Average Loss 2.304 and Accuracy: 9%\n",
            "1401 steps out of 100 steps: Average Loss 2.300 and Accuracy: 7%\n",
            "Epoch 2 ->\n",
            "1 steps out of 100 steps: Average Loss 0.000 and Accuracy: 0%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if __name__ == '__main__':\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "101 steps out of 100 steps: Average Loss 2.303 and Accuracy: 10%\n",
            "201 steps out of 100 steps: Average Loss 2.303 and Accuracy: 12%\n",
            "301 steps out of 100 steps: Average Loss 2.296 and Accuracy: 13%\n",
            "401 steps out of 100 steps: Average Loss 2.295 and Accuracy: 12%\n",
            "501 steps out of 100 steps: Average Loss 2.297 and Accuracy: 11%\n",
            "601 steps out of 100 steps: Average Loss 2.311 and Accuracy: 7%\n",
            "701 steps out of 100 steps: Average Loss 2.303 and Accuracy: 10%\n",
            "801 steps out of 100 steps: Average Loss 2.292 and Accuracy: 11%\n",
            "901 steps out of 100 steps: Average Loss 2.300 and Accuracy: 8%\n",
            "1001 steps out of 100 steps: Average Loss 2.307 and Accuracy: 8%\n",
            "1101 steps out of 100 steps: Average Loss 2.299 and Accuracy: 14%\n",
            "1201 steps out of 100 steps: Average Loss 2.294 and Accuracy: 11%\n",
            "1301 steps out of 100 steps: Average Loss 2.300 and Accuracy: 9%\n",
            "1401 steps out of 100 steps: Average Loss 2.300 and Accuracy: 4%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_I-SPiDFVXTA",
        "outputId": "bcb6853d-198f-4864-bd45-f57186eaa715"
      },
      "source": [
        "print('\\n--- Testing the CNN ---')\n",
        "loss = 0\n",
        "num_correct = 0\n",
        "for im, label in zip(test_images, test_labels):\n",
        "  _, l, acc = cnn_forward_prop(im, label)\n",
        "  loss += l\n",
        "  num_correct += acc\n",
        "\n",
        "num_tests = len(test_images)\n",
        "print('Test Loss:', loss / num_tests)\n",
        "print('Test Accuracy:', num_correct / num_tests)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Testing the CNN ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  app.launch_new_instance()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: tensor(17.8416)\n",
            "Test Accuracy: tensor(0.0960)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mC6zwdudWpZm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}